{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"..\\\\Thesis_Code\"\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = \"data_h\"\n",
    "#data_folder = \"data_e\"\n",
    "#data_folder = \"data_n\"\n",
    "#data_folder = \"data_r\"\n",
    "#data_folder = \"data_i\"\n",
    "#data_folder = \"data_hen\"\n",
    "#data_folder = \"data_ri\"\n",
    "data_folder = \"data_henri\"\n",
    "\n",
    "\n",
    "train_path = Path.cwd() / \"CNN_classifier\" / \"selector\" / data_folder / \"train\"\n",
    "val_path = Path.cwd() / \"CNN_classifier\" / \"selector\" / data_folder / \"validation\"\n",
    "test_path = Path.cwd() / \"CNN_classifier\" / \"selector\" / data_folder / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data access\n",
    "\n",
    "tsfm = transforms.Compose([transforms.Resize((224,224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = ImageFolder(root=train_path,\n",
    "                            transform=tsfm)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=25,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_dataset = ImageFolder(root=val_path,\n",
    "                          transform=tsfm)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=25,\n",
    "                        num_workers=4)\n",
    "\n",
    "train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_data_loader, val_dataset, model, loss_fn):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for b_x, b_y in val_data_loader:\n",
    "            \n",
    "            b_x, b_y = b_x.to(device), b_y.to(device)\n",
    "            \n",
    "            pred = model(b_x)\n",
    "            loss = loss_fn(pred, b_y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            n_correct += torch.sum(hard_preds == b_y).item()\n",
    "        val_accuracy = n_correct/len(val_dataset)\n",
    "        val_avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "    return val_accuracy, val_avg_loss\n",
    "\n",
    "def train_model(model, optimizer, train_loader, val_loader, train_dataset, val_dataset, epochs=10, display=True):\n",
    "\n",
    "    class_weights = torch.tensor([2359/2359, 2359/1241], dtype=torch.float)\n",
    "    loss_fn = nn.NLLLoss(weight=class_weights)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        n_correct = 0\n",
    "        \n",
    "        for i, (b_x, b_y) in enumerate(train_loader):\n",
    "            \n",
    "            b_x, b_y = b_x.to(device), b_y.to(device)\n",
    "            \n",
    "            # Compute predictions and losses\n",
    "            pred = model(b_x)\n",
    "            loss = loss_fn(pred, b_y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Count number of correct predictions\n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            n_correct += torch.sum(hard_preds == b_y).item()\n",
    "    \n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()    \n",
    "            \n",
    "            # Optionally display progress\n",
    "            if display and ((i+1)%50 == 0):\n",
    "                print(\"Batch {}\".format(i+1))\n",
    "            \n",
    "        # Compute accuracy and loss in the entire training set\n",
    "        train_accuracy = n_correct/len(train_dataset)\n",
    "        train_avg_loss = sum(losses)/len(losses)   \n",
    "            \n",
    "        # Compute accuracy and loss in the entire validation set\n",
    "        val_accuracy, val_avg_loss = evaluate_model(val_loader, val_dataset, model, loss_fn)\n",
    "            \n",
    "        # Optionally display metrics\n",
    "        if display:\n",
    "            display_str = 'Epoch {} '\n",
    "            display_str += '\\tLoss: {:.3f} '\n",
    "            display_str += '\\tLoss (val): {:.3f}'\n",
    "            display_str += '\\tAccuracy: {:.2f} '\n",
    "            display_str += '\\tAccuracy (val): {:.2f}'\n",
    "            print(display_str.format(epoch, train_avg_loss, val_avg_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "num_ftrs = alexnet.classifier[1].in_features\n",
    "num_classes = 2\n",
    "\n",
    "new_top = nn.Sequential(nn.Linear(num_ftrs, num_classes),\n",
    "                        nn.LogSoftmax(dim=1))\n",
    "\n",
    "alexnet.classifier = new_top\n",
    "\n",
    "# Freeze bottom\n",
    "for parameter in alexnet.features.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "alexnet.to(device)\n",
    "\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=1e-4)\n",
    "\n",
    "train_model(alexnet, optimizer, train_loader, val_loader, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_test_transforms(inp):\n",
    "    out = transforms.functional.resize(inp, [224,224])\n",
    "    out = transforms.functional.to_tensor(out)\n",
    "    out = transforms.functional.normalize(out, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_files = os.listdir(test_path)\n",
    "\n",
    "csv_file_path = \"./MAPF_Framework/solver/\" + data_folder + \"/\" + \"test_data.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "test_data = df.set_index(\"file\").to_dict(orient = 'index')\n",
    "\n",
    "b_count = 0\n",
    "total_costs = {\n",
    "  \"A\": 0,\n",
    "  \"B\": 0,\n",
    "  \"model\": 0,\n",
    "  \"best\": 0,\n",
    "  \"worst\": 0\n",
    "}\n",
    "\n",
    "# Make predictions\n",
    "for i, file in enumerate(test_data_files):\n",
    "    instance_name = file.split('.')[0]\n",
    "    \n",
    "    x = apply_test_transforms(Image.open(f'{test_path}/{file}').convert('RGB'))[None, :, :, :]\n",
    "    x = x.to(device)\n",
    "        \n",
    "    pred = alexnet(x)\n",
    "    hard_pred = pred.argmax(dim=1)\n",
    "    \n",
    "    pred_class = \"A\" if (hard_pred.item() == 0) else \"B\"\n",
    "    \n",
    "    if (pred_class == \"B\"):\n",
    "        b_count += 1\n",
    "    \n",
    "    total_costs[\"A\"] += test_data[instance_name][\"A\"]\n",
    "    total_costs[\"B\"] += test_data[instance_name][\"B\"]\n",
    "    total_costs[\"model\"] += test_data[instance_name][pred_class]\n",
    "    total_costs[\"best\"] += min(test_data[instance_name][\"A\"], test_data[instance_name][\"B\"])\n",
    "    total_costs[\"worst\"] += max(test_data[instance_name][\"A\"], test_data[instance_name][\"B\"])\n",
    "    \n",
    "    if (i%10 == 0):\n",
    "        print(\"Done {:5d}/{}\".format(i, len(test_data_files)))\n",
    "\n",
    "print(b_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_total_costs = {k: v-total_costs[\"best\"] for k, v in total_costs.items()}\n",
    "diff_total_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_total_costs = {k: round(100*v/total_costs[\"best\"], 2) for k, v in total_costs.items()}\n",
    "ratio_total_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_titles = [r'$\\pi_{A}$', r'$\\pi_{B}$', r'$\\pi$', \"Worst\"]\n",
    "\n",
    "def make_graph_diff(costs_in):\n",
    "    costs = dict((key,value) for key, value in costs_in.items() if key != \"best\")\n",
    "    \n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    \n",
    "    plt.title(\"Exp. HENRI\")\n",
    "    plt.xlabel(\"mappings\")\n",
    "    plt.ylabel(\"difference with \" + r\"T($\\pi^{*}$)\")\n",
    "    \n",
    "    plt.bar(range(len(costs)), list(costs.values()), align=\"center\", width = 0.6, color=[\"C0\", \"C0\", \"C2\", \"C0\"])\n",
    "    plt.xticks(range(len(costs)), x_titles)\n",
    "    \n",
    "    costs_list = list(costs.values())\n",
    "    ymin = 0\n",
    "    ymax = 80\n",
    "    \n",
    "    plt.ylim([ymin, ymax])\n",
    "    plt.grid()\n",
    "    #plt.show()\n",
    "    plt.savefig('result_exp_henri.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph_diff(diff_total_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze bottom to fine-tune\n",
    "\n",
    "for parameter in alexnet.features.parameters():\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(alexnet.parameters(), lr=1e-6)\n",
    "\n",
    "train_model(alexnet, optimizer, train_loader, val_loader, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_count_ft = 0\n",
    "total_costs_ft = {\n",
    "  \"A\": 0,\n",
    "  \"B\": 0,\n",
    "  \"model\": 0,\n",
    "  \"best\": 0,\n",
    "  \"worst\": 0\n",
    "}\n",
    "\n",
    "# Make predictions\n",
    "for i, file in enumerate(test_data_files):\n",
    "    instance_name = file.split('.')[0]\n",
    "    \n",
    "    x = apply_test_transforms(Image.open(f'{test_path}/{file}').convert('RGB'))[None, :, :, :]\n",
    "    x = x.to(device)\n",
    "        \n",
    "    pred = alexnet(x)\n",
    "    hard_pred = pred.argmax(dim=1)\n",
    "    \n",
    "    pred_class = \"A\" if (hard_pred.item() == 0) else \"B\"\n",
    "    \n",
    "    if (pred_class == \"B\"):\n",
    "        b_count_ft += 1\n",
    "    \n",
    "    total_costs_ft[\"A\"] += test_data[instance_name][\"A\"]\n",
    "    total_costs_ft[\"B\"] += test_data[instance_name][\"B\"]\n",
    "    total_costs_ft[\"model\"] += test_data[instance_name][pred_class]\n",
    "    total_costs_ft[\"best\"] += min(test_data[instance_name][\"A\"], test_data[instance_name][\"B\"])\n",
    "    total_costs_ft[\"worst\"] += max(test_data[instance_name][\"A\"], test_data[instance_name][\"B\"])\n",
    "    \n",
    "    if (i%10 == 0):\n",
    "        print(\"Done {:5d}/{}\".format(i, len(test_data_files)))\n",
    "\n",
    "print(b_count_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_total_costs_ft = {k: v-total_costs_ft[\"best\"] for k, v in total_costs_ft.items()}\n",
    "diff_total_costs_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_total_costs_ft = {k: round(100*v/total_costs_ft[\"best\"], 2) for k, v in total_costs_ft.items()}\n",
    "ratio_total_costs_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph_diff(diff_total_costs_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
